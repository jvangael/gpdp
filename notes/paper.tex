%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2012 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2012,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

\usepackage{graphicx}
\usepackage{subfigure} 
\usepackage{natbib}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{amssymb,amsmath,bm}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand{\GP}[2]{\mathcal{GP}\left({#1},{#2}\right)}
\newcommand{\DP}[2]{\mathcal{DP}\left({#1},{#2}\right)}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage[accepted]{icml2012} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Appearing in''
% \usepackage[accepted]{icml2012}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Nonparametric Noise Models for the Gaussian Process}

\begin{document} 

\twocolumn[
\icmltitle{Nonparametric Noise models for the Gaussian Process}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2012
% package.
\icmlauthor{Ulrich Paquet}{ulripa@microsoft.com}
\icmladdress{Microsoft Research, Cambridge}
\icmlauthor{Jurgen Van Gael}{jurgen@rangespan.com}
\icmladdress{Rangespan Ltd., B131 MacMillan House, Paddington Station, London W2 1FT, UK}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{boring formatting information, machine learning, ICML}

\vskip 0.3in
]

\begin{abstract} 
Notes while developing a Gaussian Process with nonparametric noise model.
\end{abstract} 

\section{Introduction}
\label{intro}

It is sometimes really hard to tell what noise model we want to use for a GP. A specific example is quantile regression for product demand forecasting. We've got an underlying trend, perhaps with cyclical component, which we can easily model with a GP by encoding prior knowledge in the covariance matrix. Unfortunately sales data might have spike and other irregularities which make choosing a noise model quite tricky. One option is to use a robust noise model like student-t or Laplace. In this work, we learn a noise model by using a non-parametric mixture of Gaussians.



\section{Model}
\label{model}

Imagine we have a time series with observations $y_t$ at times $x_t$ with $t \in [0,T]$. We model this data by assuming a latent Gaussian process
\begin{equation}
\bm{f} \sim \GP{0}{\bm{K}(\bm{x},\bm{x})}
\end{equation}
We model the noise as a non-parametric mixture model using the Dirichlet process. Let
\begin{equation}
\bm{G} \sim \DP{\alpha}{H}
\end{equation}
be a Dirichlet process with concentration parameter $\alpha$ and base measure $H$. For each time $t$ we introduce a noise variable $\epsilon_t \sim G$. We then model the observation $y_t = \bm{f}(x_t) + \epsilon_t$. In this work we restrict ourselves to the case where $H$ is a Normal-Inverse Gamma distribution to parameterize the mean and variance of a normal mixture component.



\section{Inference}
\label{inference}

We can perform inference in this model using a collapsed Gibbs sampler. In order to work with the CRP representation of the Dirichlet process we introduce a new variable $z_t$ which will represent CRP partition that datapoint $t$ belongs to. For each CRP partition $n$ we represent the cluster parameters using $(\mu_n, \sigma^2_n) = \theta_n$.

\begin{algorithm}[tb]
   \caption{Collapsed Gibbs Sampling}
   \label{alg:cgibbs}
\begin{algorithmic}
   \STATE {\bfseries Input:} data $\bm{x}, \bm{y}$ and kernel $\bm{K}(\bm{x},\bm{x})$
   \STATE {\bfseries Initialisation:} $z_t \sim \textrm{CRP}(\alpha)$, $\theta_n \sim H$
   \REPEAT
   \STATE Sample $z_t | \bm{K}, \bm{y}, \bm{z}_{\lnot t}, \bm{\theta}$
   \STATE Sample $\theta_n | \bm{x}, \bm{y}, \bm{\theta}$
   \UNTIL convergence
\end{algorithmic}
\end{algorithm}

In algorithm~\ref{alg:cgibbs} we integrate out the Gaussian process $\bm{f}$. In what follows we derive the resampling steps for $z_t$ and $\theta_n$.

\paragraph{Sampling $z_t$} The conditional distribution of $z_t$ can be written as follows
\begin{equation}
p(z_t | \bm{K}, \bm{y}, \bm{z}_{\lnot t}, \bm{\theta}, \alpha) \propto p(z_t | \bm{z}_{\lnot t}, \alpha) p(\bm{y} | \bm{z}, \bm{K}, \bm{\theta}).
\end{equation}

We can compute $p(z_t | \bm{z}_{\lnot t}$ easily using the standard conditional probability for a CRP. Given the noise parameters for every CRP partition are fixed, we can compute the marginal likelihood $p(\bm{y} | \bm{z}, \bm{K}, \bm{\theta})$ analytically. (Equation 2.30 in Carl's book).

NOTE: we need to say something about sampling a new $z_t$, algorithm 8 from Radford Neal's paper should be able to make this happen.


\paragraph{Sampling $\bm{\theta}$}
The condition distribution of $\bm{\theta}$ can be written as follows
\begin{equation}
p(\bm{\theta} | \bm{K}, \bm{y}, \bm{z}, H) \propto p(\bm{\theta} | H) p(\bm{y} | \bm{z}, \bm{K}. \bm{\theta})
\end{equation}
Unfortunately $\bm{\theta}$ influences the marginal likelihood in a complicated way which implies that an analytical expression for the condition distribution on $\bm{\theta}$ is impossible.

\subparagraph{Metropolis-Hastings}
The simplest and probably least efficient method to sample form the conditional distribution of $\bm{\theta}$ is to use Metropolis-Hastings sampling. We create a proposal distribution around $\bm{\theta}$ and can then evaluate the conditional probability up to a proportionality constant by computing the marginal likelihood of the observations.

\subparagraph{Auxiliary Variable}
An alternative method to sample from the conditional distribution on $\bm{\theta}$ is to introduce a latent variable $\bm{f}$ for the Gaussian Process. First, we sample
\begin{equation}
p(\bm{f} | \bm{\theta}, \bm{K}, \bm{y}, \bm{z}, H),
\end{equation}
from a multivariate normal distribution.

Given the latent variable $\bm{f}$, each CRP partition becomes independent, and we can sample the $\bm{\theta}$.
\begin{equation}
p(\bm{\theta} | \bm{f}, \bm{K}, \bm{y}, \bm{z}, H) \propto \prod_k p(\theta_k) \prod_n p(y_n | f_n, \theta_{z_n}).
\end{equation}

NOTE: Can we analytically compute the posterior of $\mu,\sigma$? It's learning a number of Normal-Inverse Gamma distributions where the observations are Gaussians (the GP).






\subsection{Software and Data}


% Acknowledgements should only appear in the accepted version. 
\section*{Acknowledgments} 

\bibliography{paper}
\bibliographystyle{icml2012}

\end{document}

